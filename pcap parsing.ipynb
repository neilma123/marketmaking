{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a5a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyshark\n",
      "  Downloading pyshark-0.6-py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\nmasc\\anaconda3\\lib\\site-packages (from pyshark) (1.1.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\nmasc\\anaconda3\\lib\\site-packages (from pyshark) (1.4.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\nmasc\\anaconda3\\lib\\site-packages (from pyshark) (4.6.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\nmasc\\anaconda3\\lib\\site-packages (from pyshark) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nmasc\\anaconda3\\lib\\site-packages (from packaging->pyshark) (3.0.4)\n",
      "Installing collected packages: pyshark\n",
      "Successfully installed pyshark-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pyshark\n",
    "\n",
    "# out_00000_20210719001654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998f1afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot run the event loop while another loop is running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6384/270376844.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Run the asynchronous function using asyncio's run method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpcap_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'out_00000_20210719001654.pcap'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mawait\u001b[0m \u001b[0mread_pcap_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcap_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6384/270376844.py\u001b[0m in \u001b[0;36mread_pcap_async\u001b[1;34m(pcap_file)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Iterate through packets asynchronously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mpacket\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacket\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# You can access packet details here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyshark\\capture\\capture.py\u001b[0m in \u001b[0;36m_packets_from_tshark_sync\u001b[1;34m(self, packet_count, existing_process)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \"\"\"\n\u001b[0;32m    211\u001b[0m         \u001b[1;31m# NOTE: This has code duplication with the async version, think about how to solve this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         tshark_process = existing_process or self.eventloop.run_until_complete(\n\u001b[0m\u001b[0;32m    213\u001b[0m             self._get_tshark_process())\n\u001b[0;32m    214\u001b[0m         \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_tshark_output_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \"\"\"\n\u001b[0;32m    591\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This event loop is already running'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    555\u001b[0m                 'Cannot run the event loop while another loop is running')\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot run the event loop while another loop is running"
     ]
    }
   ],
   "source": [
    "import pyshark\n",
    "import asyncio\n",
    "\n",
    "# Define an async function to capture packets\n",
    "async def read_pcap_async(pcap_file):\n",
    "    cap = pyshark.FileCapture(pcap_file)\n",
    "    \n",
    "    # Iterate through packets asynchronously\n",
    "    for packet in cap:\n",
    "        print(packet)  # You can access packet details here\n",
    "\n",
    "    # Close capture when done\n",
    "    await cap.close()\n",
    "\n",
    "# Run the asynchronous function using asyncio's run method\n",
    "pcap_file = 'out_00000_20210719001654.pcap'\n",
    "await read_pcap_async(pcap_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06392b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import rdpcap, Raw\n",
    "from scapy.layers.inet import IP, UDP\n",
    "import struct\n",
    "def parse_arcabook_packet(data):\n",
    "    msg_size = struct.unpack('<H', data[0:2])[0]\n",
    "    msg_type = struct.unpack('<H', data[2:4])[0]\n",
    "    \n",
    "    parsed_data = {\n",
    "        'Msg Size': msg_size,\n",
    "        'Msg Type': msg_type\n",
    "    }\n",
    "    \n",
    "    if msg_type == 100:  # Add Order Message\n",
    "        parsed_data.update({\n",
    "            'SourceTimeNS': struct.unpack('<I', data[4:8])[0],\n",
    "            'SymbolIndex': struct.unpack('<I', data[8:12])[0],\n",
    "            'SymbolSeqNum': struct.unpack('<I', data[12:16])[0],\n",
    "            'OrderID': struct.unpack('<I', data[16:20])[0],\n",
    "            'Price': struct.unpack('<I', data[20:24])[0],\n",
    "            'Volume': struct.unpack('<I', data[24:28])[0],\n",
    "            'Side': data[28:29].decode('ascii'),\n",
    "            'OrderIDGTCIndicator': data[29],\n",
    "            'TradeSession': data[30]\n",
    "        })\n",
    "    elif msg_type == 101:  # Modify Message\n",
    "        parsed_data.update({\n",
    "            'SourceTimeNS': struct.unpack('<I', data[4:8])[0],\n",
    "            'SymbolIndex': struct.unpack('<I', data[8:12])[0],\n",
    "            'SymbolSeqNum': struct.unpack('<I', data[12:16])[0],\n",
    "            'OrderID': struct.unpack('<I', data[16:20])[0],\n",
    "            'Price': struct.unpack('<I', data[20:24])[0],\n",
    "            'Volume': struct.unpack('<I', data[24:28])[0],\n",
    "            'Side': data[28:29].decode('ascii'),\n",
    "            'OrderIDGTCIndicator': data[29],\n",
    "            'ReasonCode': data[30]\n",
    "        })\n",
    "    elif msg_type == 102:  # Delete Message\n",
    "        parsed_data.update({\n",
    "            'SourceTimeNS': struct.unpack('<I', data[4:8])[0],\n",
    "            'SymbolIndex': struct.unpack('<I', data[8:12])[0],\n",
    "            'SymbolSeqNum': struct.unpack('<I', data[12:16])[0],\n",
    "            'OrderID': struct.unpack('<I', data[16:20])[0],\n",
    "            'Side': data[20:21].decode('ascii'),\n",
    "            'OrderIDGTCIndicator': data[21],\n",
    "            'ReasonCode': data[22]\n",
    "        })\n",
    "    elif msg_type == 103:  # Execution Message\n",
    "        parsed_data.update({\n",
    "            'SourceTimeNS': struct.unpack('<I', data[4:8])[0],\n",
    "            'SymbolIndex': struct.unpack('<I', data[8:12])[0],\n",
    "            'SymbolSeqNum': struct.unpack('<I', data[12:16])[0],\n",
    "            'OrderID': struct.unpack('<I', data[16:20])[0],\n",
    "            'Price': struct.unpack('<I', data[20:24])[0],\n",
    "            'Volume': struct.unpack('<I', data[24:28])[0],\n",
    "            'OrderIDGTCIndicator': data[28],\n",
    "            'ReasonCode': data[29],\n",
    "            'TradeID': struct.unpack('<I', data[30:34])[0]\n",
    "        })\n",
    "    # Add more message types as needed\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "# Specify the path to your PCAP file\n",
    "\n",
    "\n",
    "pcap_directory = 'C:\\\\Users\\\\nmasc\\\\Documents\\\\para\\\\projects\\\\QuantEdge\\\\marketmaking\\\\NYSEBQT_CVOLCTS_PUB_A_07_19_2021_mid_8pm'\n",
    "import os\n",
    "\n",
    "# Loop over each file in the directory\n",
    "for filename in os.listdir(pcap_directory):\n",
    "    if filename.endswith('.pcap'):\n",
    "        pcap_file = filename\n",
    "\n",
    "        # Read the PCAP file\n",
    "        packets = rdpcap(pcap_file)\n",
    "        # Iterate through the packets and print details\n",
    "        for packet in packets:\n",
    "            # If the packet contains a UDP layer, extract specific details\n",
    "            \n",
    "            if UDP in packet:\n",
    "                # Access IP layer\n",
    "                ip_layer = packet[IP]\n",
    "                src_ip = ip_layer.src\n",
    "                dst_ip = ip_layer.dst\n",
    "\n",
    "                # Access UDP layer\n",
    "                udp_layer = packet[UDP]\n",
    "                src_port = udp_layer.sport\n",
    "                dst_port = udp_layer.dport\n",
    "\n",
    "\n",
    "                # If there's a Raw layer, convert the payload to decimal\n",
    "                if Raw in packet:\n",
    "                    raw_data = packet[Raw].load  # Extract raw payload\n",
    "                    decimal_values = [byte for byte in raw_data]  # Convert bytes to decimal\n",
    "                    if decimal_values[2] != 1 and int(decimal_values[2]) < 200:\n",
    "                        parsed_packet = parse_arcabook_packet(raw_data)\n",
    "                        if parsed_packet['Msg Type'] != 267 and parsed_packet['Msg Type'] != 268:\n",
    "                            print(parsed_packet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5950047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import *\n",
    "import struct\n",
    "\n",
    "def process_pcap(pcap_file):\n",
    "    packets = rdpcap(pcap_file)\n",
    "    \n",
    "    for packet in packets:\n",
    "        if \"UDP\" in packet.summary():\n",
    "            continue\n",
    "        else:\n",
    "            print(packet.summary())\n",
    "\n",
    "# Replace 'your_file.pcap' with the path to your PCAP file\n",
    "pcap_directory = 'C:\\\\Users\\\\nmasc\\\\Documents\\\\para\\\\projects\\\\QuantEdge\\\\marketmaking\\\\NYSEBQT_CVOLCTS_PUB_A_07_19_2021_mid_8pm'\n",
    "import os\n",
    "\n",
    "# Loop over each file in the directory\n",
    "for filename in os.listdir(pcap_directory):\n",
    "    if filename.endswith('.pcap'):\n",
    "        pcap_file = filename\n",
    "        process_pcap(pcap_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf88aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import *\n",
    "import csv\n",
    "\n",
    "# Define message types and field mappings based on spec\n",
    "message_types = {\n",
    "    '100': 'Add Order',\n",
    "    '101': 'Modify Order',\n",
    "    '102': 'Delete Order',\n",
    "    '103': 'Execution',\n",
    "    '105': 'Imbalance',\n",
    "    '106': 'Add Order Refresh',\n",
    "    '107': 'Attributed Add Order', \n",
    "    '108': 'Attributed Add Order Refresh'\n",
    "}\n",
    "\n",
    "field_mappings = {\n",
    "    '100': ['MsgSize', 'MsgType', 'SourceTimeNS', 'SymbolIndex', 'SymbolSeqNum', 'OrderID', 'Price', 'Volume', 'Side', 'OrderIDGTCIndicator', 'TradeSession'],\n",
    "    # Define mappings for other message types\n",
    "}\n",
    "\n",
    "# Read PCAP file\n",
    "packets = rdpcap(\"onepacket.pcap\")\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow(['Timestamp', 'MessageType'] + field_mappings['100'])  # Using '100' fields as example\n",
    "    \n",
    "    # Process each packet\n",
    "    for packet in packets:\n",
    "        if TCP in packet and packet[TCP].payload:\n",
    "            data = bytes(packet[TCP].payload)\n",
    "            \n",
    "            # Extract message type\n",
    "            msg_type = data[2:4].decode()\n",
    "            \n",
    "            if msg_type in message_types:\n",
    "                # Extract fields based on message type\n",
    "                fields = field_mappings[msg_type]\n",
    "                values = [packet.time, message_types[msg_type]]\n",
    "                \n",
    "                for field in fields:\n",
    "                    # Extract value for each field\n",
    "                    # This would require implementing logic for each field type\n",
    "                    values.append(extract_field(data, field))\n",
    "                \n",
    "                # Write row to CSV\n",
    "                writer.writerow(values)\n",
    "\n",
    "def extract_field(data, field):\n",
    "    # Implement extraction logic for each field type\n",
    "    # This would be based on the field definitions in the spec\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
